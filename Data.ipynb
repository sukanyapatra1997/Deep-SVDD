{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1afefbf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/e/Codes/Deep-SVDD/src\n"
     ]
    }
   ],
   "source": [
    "%cd src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bafd3add",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.main import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d676c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'custom'\n",
    "data_path = '../data/data_20.npy' \n",
    "normal_class= 3\n",
    "dataset_custom = load_dataset(dataset_name, data_path, normal_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93043411",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'mnist'\n",
    "data_path = '../data' \n",
    "normal_class= 3\n",
    "dataset_mnist = load_dataset(dataset_name, data_path, normal_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be91c2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_mnist[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19d8bf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfec3729",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ee17cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data = np.load('/mnt/e/FLARACC/Sukanya_data/data_20.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0f1b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data=total_data[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fedbb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf49dfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FaceLandmarksDataset(TorchvisionDataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        #return len(self.landmarks_frame)\n",
    "        return total_data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "#         print(total_data.shape)\n",
    "#         print(total_data[idx].shape)\n",
    "        sample=total_data[idx]\n",
    "        sample = np.array([sample])\n",
    "        sample = sample.astype('float')\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26fd041d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = FaceLandmarksDataset(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35df39ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=10,\n",
    "                        shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d77f46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c956663",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in dataloader:\n",
    "    print(len(x))\n",
    "    print(x.shape)\n",
    "    print(x[0].shape)\n",
    "    print(x[1].shape)\n",
    "    print(x[2].shape)\n",
    "    print(x[1])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89967186",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.preprocessing import get_target_label_idx, global_contrast_normalization\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Lambda(lambda x: global_contrast_normalization(x, scale='l1'))\n",
    "                               ])\n",
    "train_set = FaceLandmarksDataset(transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4941d06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataLoader = DataLoader(train_set, batch_size=10,\n",
    "                        shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2866854d",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterCount = 0\n",
    "datasetValues = {}\n",
    "\n",
    "for data in dataLoader:\n",
    "    if iterCount == 0:\n",
    "        datasetValues['currDataset'] = data.clone()\n",
    "    else:\n",
    "        datasetValues['currDataset'] = torch.cat((datasetValues['currDataset'], data.clone()), 0)\n",
    "\n",
    "    iterCount += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "80eda081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 608, 1, 184])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasetValues['currDataset'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "50b79413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-4.2406, dtype=torch.float64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasetValues['currDataset'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eb648c0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.9455, dtype=torch.float64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasetValues['currDataset'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d474ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89d910f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in dataLoader:\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677ed3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torch.empty((608,1,184))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b870c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5f200a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.reshape(1,184,608).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c0e7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loaders, _ = dataset_mnist.loaders(batch_size=10,\n",
    "                        num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c734a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "from base.torchvision_dataset import TorchvisionDataset\n",
    "from datasets.preprocessing import get_target_label_idx, global_contrast_normalization\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "class Custom_Dataset(TorchvisionDataset):\n",
    "\n",
    "    def __init__(self, root: str, normal_class=0):\n",
    "        super().__init__(root)\n",
    "\n",
    "        # Pre-computed min and max values (after applying GCN) from train data per class\n",
    "        min_max = [-8.5016, 7.0120]\n",
    "\n",
    "        # Custom dataset preprocessing: GCN (with L1 norm) and min-max feature scaling to [0,1]\n",
    "        transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                        transforms.Lambda(lambda x: global_contrast_normalization(x, scale='l1')),\n",
    "                                        transforms.Normalize([min_max[0]],\n",
    "                                                             [min_max[1] - min_max[0]])])\n",
    "\n",
    "    \n",
    "        self.train_set = Custom(root = self.root, transform=transform)\n",
    "        self.test_set=None\n",
    "\n",
    "    def loaders(self, batch_size: int, shuffle_train=True, shuffle_test=False, num_workers: int = 0):\n",
    "        \"\"\"Implement data loaders of type torch.utils.data.DataLoader for train_set and test_set.\"\"\"\n",
    "\n",
    "        print(f'train_set: {len(self.train_set)}')\n",
    "\n",
    "        train_loader = DataLoader(self.train_set, batch_size=batch_size,\n",
    "                        shuffle=shuffle_train, num_workers=num_workers)\n",
    "        # test_dataLoader = DataLoader(self.test_set, batch_size=batch_size,\n",
    "        #                 shuffle=shuffle_test, num_workers=num_workers)\n",
    "\n",
    "        test_loader = train_loader\n",
    "        print(\"***********in Loader***************\")\n",
    "        return train_loader, test_loader\n",
    "\n",
    "class Custom(Dataset):\n",
    "    \"\"\"FLARACC dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.data = np.load(root)\n",
    "        self.transform = transform\n",
    "        print(f'data Shape: {self.data.shape}')\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        print(f'idx: {idx}')\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        sample=self.data[idx]\n",
    "        sample = np.array([sample])\n",
    "        sample = sample.astype('float')\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample).reshape(1,184,608)\n",
    "\n",
    "        print(f'sample shape: {sample.shape}')\n",
    "        return sample, [0], [0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a889d98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data Shape: (20, 184, 608)\n"
     ]
    }
   ],
   "source": [
    "dataset=Custom_Dataset(root='../data/data_20.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e8b719b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set: 20\n",
      "***********in Loader***************\n"
     ]
    }
   ],
   "source": [
    "train_loader, _ = dataset.loaders(batch_size=10,\n",
    "                        shuffle_train=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "295b14d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx: 11\n",
      "sample shape: torch.Size([1, 184, 608])\n",
      "idx: 13\n",
      "sample shape: torch.Size([1, 184, 608])\n",
      "idx: 8\n",
      "sample shape: torch.Size([1, 184, 608])\n",
      "idx: 2\n",
      "sample shape: torch.Size([1, 184, 608])\n",
      "idx: 14\n",
      "sample shape: torch.Size([1, 184, 608])\n",
      "idx: 0\n",
      "sample shape: torch.Size([1, 184, 608])\n",
      "idx: 3\n",
      "sample shape: torch.Size([1, 184, 608])\n",
      "idx: 7\n",
      "sample shape: torch.Size([1, 184, 608])\n",
      "idx: 19\n",
      "sample shape: torch.Size([1, 184, 608])\n",
      "idx: 1\n",
      "sample shape: torch.Size([1, 184, 608])\n",
      "[tensor([[[[ 0.4735,  0.4746,  0.4757,  ...,  0.5420,  0.5420,  0.5593],\n",
      "          [ 0.5593,  0.5766,  0.5766,  ...,  0.7152,  0.7152,  0.7152],\n",
      "          [ 0.7326,  0.7326,  0.7326,  ..., -0.0645, -0.0992, -0.1338],\n",
      "          ...,\n",
      "          [-1.2428, -1.2255, -1.1735,  ..., -0.1512, -0.1512, -0.1512],\n",
      "          [-0.1512, -0.1685, -0.1685,  ..., -0.4284, -0.4284, -0.4284],\n",
      "          [-0.4457, -0.4457, -0.4631,  ..., -2.2478, -2.2652, -2.2652]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4735,  0.4746,  0.4757,  ...,  0.5420,  0.5420,  0.5593],\n",
      "          [ 0.5593,  0.5766,  0.5766,  ...,  0.7152,  0.7152,  0.7152],\n",
      "          [ 0.7326,  0.7326,  0.7326,  ..., -0.0645, -0.0992, -0.1338],\n",
      "          ...,\n",
      "          [-1.2428, -1.2255, -1.1735,  ..., -0.1512, -0.1512, -0.1512],\n",
      "          [-0.1512, -0.1685, -0.1685,  ..., -0.4284, -0.4284, -0.4284],\n",
      "          [-0.4457, -0.4457, -0.4631,  ..., -2.2478, -2.2652, -2.2652]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4735,  0.4746,  0.4757,  ...,  0.5420,  0.5420,  0.5593],\n",
      "          [ 0.5593,  0.5766,  0.5766,  ...,  0.7152,  0.7152,  0.7152],\n",
      "          [ 0.7326,  0.7326,  0.7326,  ..., -0.0645, -0.0992, -0.1338],\n",
      "          ...,\n",
      "          [-1.2428, -1.2255, -1.1735,  ..., -0.1512, -0.1512, -0.1512],\n",
      "          [-0.1512, -0.1685, -0.1685,  ..., -0.4284, -0.4284, -0.4284],\n",
      "          [-0.4457, -0.4457, -0.4631,  ..., -2.2478, -2.2652, -2.2652]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.4735,  0.4746,  0.4757,  ...,  0.5420,  0.5420,  0.5593],\n",
      "          [ 0.5593,  0.5766,  0.5766,  ...,  0.7152,  0.7152,  0.7152],\n",
      "          [ 0.7326,  0.7326,  0.7326,  ..., -0.0645, -0.0992, -0.1338],\n",
      "          ...,\n",
      "          [-1.2428, -1.2255, -1.1735,  ..., -0.1512, -0.1512, -0.1512],\n",
      "          [-0.1512, -0.1685, -0.1685,  ..., -0.4284, -0.4284, -0.4284],\n",
      "          [-0.4457, -0.4457, -0.4631,  ..., -2.2478, -2.2652, -2.2652]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4735,  0.4746,  0.4757,  ...,  0.5420,  0.5420,  0.5593],\n",
      "          [ 0.5593,  0.5766,  0.5766,  ...,  0.7152,  0.7152,  0.7152],\n",
      "          [ 0.7326,  0.7326,  0.7326,  ..., -0.0645, -0.0992, -0.1338],\n",
      "          ...,\n",
      "          [-1.2428, -1.2255, -1.1735,  ..., -0.1512, -0.1512, -0.1512],\n",
      "          [-0.1512, -0.1685, -0.1685,  ..., -0.4284, -0.4284, -0.4284],\n",
      "          [-0.4457, -0.4457, -0.4631,  ..., -2.2478, -2.2652, -2.2652]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4735,  0.4746,  0.4757,  ...,  0.5420,  0.5420,  0.5593],\n",
      "          [ 0.5593,  0.5766,  0.5766,  ...,  0.7152,  0.7152,  0.7152],\n",
      "          [ 0.7326,  0.7326,  0.7326,  ..., -0.0645, -0.0992, -0.1338],\n",
      "          ...,\n",
      "          [-1.2428, -1.2255, -1.1735,  ..., -0.1512, -0.1512, -0.1512],\n",
      "          [-0.1512, -0.1685, -0.1685,  ..., -0.4284, -0.4284, -0.4284],\n",
      "          [-0.4457, -0.4457, -0.4631,  ..., -2.2478, -2.2652, -2.2652]]]],\n",
      "       dtype=torch.float64), [tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])], [tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]]\n",
      "idx: 15\n",
      "sample shape: torch.Size([1, 184, 608])\n",
      "idx: 10\n",
      "sample shape: torch.Size([1, 184, 608])\n",
      "idx: 18\n",
      "sample shape: torch.Size([1, 184, 608])\n",
      "idx: 9\n",
      "sample shape: torch.Size([1, 184, 608])\n",
      "idx: 12\n",
      "sample shape: torch.Size([1, 184, 608])\n",
      "idx: 17\n",
      "sample shape: torch.Size([1, 184, 608])\n",
      "idx: 5\n",
      "sample shape: torch.Size([1, 184, 608])\n",
      "idx: 16\n",
      "sample shape: torch.Size([1, 184, 608])\n",
      "idx: 4\n",
      "sample shape: torch.Size([1, 184, 608])\n",
      "idx: 6\n",
      "sample shape: torch.Size([1, 184, 608])\n",
      "[tensor([[[[ 0.4735,  0.4746,  0.4757,  ...,  0.5420,  0.5420,  0.5593],\n",
      "          [ 0.5593,  0.5766,  0.5766,  ...,  0.7152,  0.7152,  0.7152],\n",
      "          [ 0.7326,  0.7326,  0.7326,  ..., -0.0645, -0.0992, -0.1338],\n",
      "          ...,\n",
      "          [-1.2428, -1.2255, -1.1735,  ..., -0.1512, -0.1512, -0.1512],\n",
      "          [-0.1512, -0.1685, -0.1685,  ..., -0.4284, -0.4284, -0.4284],\n",
      "          [-0.4457, -0.4457, -0.4631,  ..., -2.2478, -2.2652, -2.2652]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4735,  0.4746,  0.4757,  ...,  0.5420,  0.5420,  0.5593],\n",
      "          [ 0.5593,  0.5766,  0.5766,  ...,  0.7152,  0.7152,  0.7152],\n",
      "          [ 0.7326,  0.7326,  0.7326,  ..., -0.0645, -0.0992, -0.1338],\n",
      "          ...,\n",
      "          [-1.2428, -1.2255, -1.1735,  ..., -0.1512, -0.1512, -0.1512],\n",
      "          [-0.1512, -0.1685, -0.1685,  ..., -0.4284, -0.4284, -0.4284],\n",
      "          [-0.4457, -0.4457, -0.4631,  ..., -2.2478, -2.2652, -2.2652]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4735,  0.4746,  0.4757,  ...,  0.5420,  0.5420,  0.5593],\n",
      "          [ 0.5593,  0.5766,  0.5766,  ...,  0.7152,  0.7152,  0.7152],\n",
      "          [ 0.7326,  0.7326,  0.7326,  ..., -0.0645, -0.0992, -0.1338],\n",
      "          ...,\n",
      "          [-1.2428, -1.2255, -1.1735,  ..., -0.1512, -0.1512, -0.1512],\n",
      "          [-0.1512, -0.1685, -0.1685,  ..., -0.4284, -0.4284, -0.4284],\n",
      "          [-0.4457, -0.4457, -0.4631,  ..., -2.2478, -2.2652, -2.2652]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.4735,  0.4746,  0.4757,  ...,  0.5420,  0.5420,  0.5593],\n",
      "          [ 0.5593,  0.5766,  0.5766,  ...,  0.7152,  0.7152,  0.7152],\n",
      "          [ 0.7326,  0.7326,  0.7326,  ..., -0.0645, -0.0992, -0.1338],\n",
      "          ...,\n",
      "          [-1.2428, -1.2255, -1.1735,  ..., -0.1512, -0.1512, -0.1512],\n",
      "          [-0.1512, -0.1685, -0.1685,  ..., -0.4284, -0.4284, -0.4284],\n",
      "          [-0.4457, -0.4457, -0.4631,  ..., -2.2478, -2.2652, -2.2652]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4735,  0.4746,  0.4757,  ...,  0.5420,  0.5420,  0.5593],\n",
      "          [ 0.5593,  0.5766,  0.5766,  ...,  0.7152,  0.7152,  0.7152],\n",
      "          [ 0.7326,  0.7326,  0.7326,  ..., -0.0645, -0.0992, -0.1338],\n",
      "          ...,\n",
      "          [-1.2428, -1.2255, -1.1735,  ..., -0.1512, -0.1512, -0.1512],\n",
      "          [-0.1512, -0.1685, -0.1685,  ..., -0.4284, -0.4284, -0.4284],\n",
      "          [-0.4457, -0.4457, -0.4631,  ..., -2.2478, -2.2652, -2.2652]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4735,  0.4746,  0.4757,  ...,  0.5420,  0.5420,  0.5593],\n",
      "          [ 0.5593,  0.5766,  0.5766,  ...,  0.7152,  0.7152,  0.7152],\n",
      "          [ 0.7326,  0.7326,  0.7326,  ..., -0.0645, -0.0992, -0.1338],\n",
      "          ...,\n",
      "          [-1.2428, -1.2255, -1.1735,  ..., -0.1512, -0.1512, -0.1512],\n",
      "          [-0.1512, -0.1685, -0.1685,  ..., -0.4284, -0.4284, -0.4284],\n",
      "          [-0.4457, -0.4457, -0.4631,  ..., -2.2478, -2.2652, -2.2652]]]],\n",
      "       dtype=torch.float64), [tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])], [tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]]\n"
     ]
    }
   ],
   "source": [
    "for data in train_loader:\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f4cfb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
