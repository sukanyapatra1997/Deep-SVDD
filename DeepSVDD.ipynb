{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DeepSVDD.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMAHT8a3XdnCxC6+WEu+x9f"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wd-UZ5OpoMa9","executionInfo":{"status":"ok","timestamp":1648154934934,"user_tz":-60,"elapsed":1030,"user":{"displayName":"sukanya patra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhA0VAl2847_nVmxDG5M6WXHDAtCA2HrGslmCXLSQ=s64","userId":"13695225565748114902"}},"outputId":"b2ca30c2-4fb8-4f8b-a38d-a923fe34bf4a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'Deep-SVDD-PyTorch'...\n","remote: Enumerating objects: 494, done.\u001b[K\n","remote: Counting objects: 100% (3/3), done.\u001b[K\n","remote: Compressing objects: 100% (3/3), done.\u001b[K\n","remote: Total 494 (delta 0), reused 0 (delta 0), pack-reused 491\u001b[K\n","Receiving objects: 100% (494/494), 2.16 MiB | 11.70 MiB/s, done.\n","Resolving deltas: 100% (321/321), done.\n"]}],"source":["! git clone https://github.com/lukasruff/Deep-SVDD-PyTorch.git"]},{"cell_type":"code","source":["%cd Deep-SVDD-PyTorch/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HJYtD3F-qAo2","executionInfo":{"status":"ok","timestamp":1648154970586,"user_tz":-60,"elapsed":2,"user":{"displayName":"sukanya patra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhA0VAl2847_nVmxDG5M6WXHDAtCA2HrGslmCXLSQ=s64","userId":"13695225565748114902"}},"outputId":"ef30541e-7c39-47e5-d4f0-38e0a02cf689"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/Deep-SVDD-PyTorch\n"]}]},{"cell_type":"code","source":["%%capture\n","!pip install -r requirements.txt"],"metadata":{"id":"nuYKhWT1qTgk","executionInfo":{"status":"ok","timestamp":1648154951627,"user_tz":-60,"elapsed":13477,"user":{"displayName":"sukanya patra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhA0VAl2847_nVmxDG5M6WXHDAtCA2HrGslmCXLSQ=s64","userId":"13695225565748114902"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["!mkdir log/mnist_test\n","%cd src"],"metadata":{"id":"2gQg31KVqYUM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648155012099,"user_tz":-60,"elapsed":178,"user":{"displayName":"sukanya patra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhA0VAl2847_nVmxDG5M6WXHDAtCA2HrGslmCXLSQ=s64","userId":"13695225565748114902"}},"outputId":"723a9b07-0fa6-4c23-f455-5dee0aaec35e"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/Deep-SVDD-PyTorch/src\n"]}]},{"cell_type":"code","source":["!python main.py mnist mnist_LeNet ../log/mnist_test ../data --objective one-class --lr 0.0001 --n_epochs 150 --lr_milestone 50 --batch_size 200 --weight_decay 0.5e-6 --pretrain True --ae_lr 0.0001 --ae_n_epochs 150 --ae_lr_milestone 50 --ae_batch_size 200 --ae_weight_decay 0.5e-3 --normal_class 3"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GupTnwiwsaly","executionInfo":{"status":"ok","timestamp":1648155998046,"user_tz":-60,"elapsed":950296,"user":{"displayName":"sukanya patra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhA0VAl2847_nVmxDG5M6WXHDAtCA2HrGslmCXLSQ=s64","userId":"13695225565748114902"}},"outputId":"d49ec7c1-6784-4306-e5c8-24164dc05e22"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["INFO:root:Log file is ../log/mnist_test/log.txt.\n","INFO:root:Data path is ../data.\n","INFO:root:Export path is ../log/mnist_test.\n","INFO:root:Dataset: mnist\n","INFO:root:Normal class: 3\n","INFO:root:Network: mnist_LeNet\n","INFO:root:Deep SVDD objective: one-class\n","INFO:root:Nu-paramerter: 0.10\n","INFO:root:Computation device: cuda\n","INFO:root:Number of dataloader workers: 0\n","Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MyMNIST/raw/train-images-idx3-ubyte.gz\n","9913344it [00:00, 71073772.51it/s]                 \n","Extracting ../data/MyMNIST/raw/train-images-idx3-ubyte.gz to ../data/MyMNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MyMNIST/raw/train-labels-idx1-ubyte.gz\n","29696it [00:00, 77992518.21it/s]\n","Extracting ../data/MyMNIST/raw/train-labels-idx1-ubyte.gz to ../data/MyMNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MyMNIST/raw/t10k-images-idx3-ubyte.gz\n","1649664it [00:00, 26161891.11it/s]\n","Extracting ../data/MyMNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MyMNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MyMNIST/raw/t10k-labels-idx1-ubyte.gz\n","5120it [00:00, 29138177.04it/s]\n","Extracting ../data/MyMNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MyMNIST/raw\n","\n","/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:52: UserWarning: train_labels has been renamed targets\n","  warnings.warn(\"train_labels has been renamed targets\")\n","INFO:root:Pretraining: True\n","INFO:root:Pretraining optimizer: adam\n","INFO:root:Pretraining learning rate: 0.0001\n","INFO:root:Pretraining epochs: 150\n","INFO:root:Pretraining learning rate scheduler milestones: (50,)\n","INFO:root:Pretraining batch size: 200\n","INFO:root:Pretraining weight decay: 0.0005\n","INFO:root:Starting pretraining...\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n","/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:62: UserWarning: train_data has been renamed data\n","  warnings.warn(\"train_data has been renamed data\")\n","INFO:root:  Epoch 1/150\t Time: 3.516\t Loss: 112.96286011\n","INFO:root:  Epoch 2/150\t Time: 3.346\t Loss: 68.98775790\n","INFO:root:  Epoch 3/150\t Time: 3.267\t Loss: 46.59669605\n","INFO:root:  Epoch 4/150\t Time: 3.331\t Loss: 34.15426027\n","INFO:root:  Epoch 5/150\t Time: 3.302\t Loss: 26.27660604\n","INFO:root:  Epoch 6/150\t Time: 3.277\t Loss: 21.46657519\n","INFO:root:  Epoch 7/150\t Time: 3.289\t Loss: 18.12479007\n","INFO:root:  Epoch 8/150\t Time: 3.278\t Loss: 15.73059894\n","INFO:root:  Epoch 9/150\t Time: 3.313\t Loss: 13.97218793\n","INFO:root:  Epoch 10/150\t Time: 3.273\t Loss: 12.62025046\n","INFO:root:  Epoch 11/150\t Time: 3.302\t Loss: 11.55347221\n","INFO:root:  Epoch 12/150\t Time: 3.330\t Loss: 10.68457336\n","INFO:root:  Epoch 13/150\t Time: 3.316\t Loss: 9.98225812\n","INFO:root:  Epoch 14/150\t Time: 3.391\t Loss: 9.39276914\n","INFO:root:  Epoch 15/150\t Time: 3.343\t Loss: 8.89230987\n","INFO:root:  Epoch 16/150\t Time: 3.285\t Loss: 8.47130369\n","INFO:root:  Epoch 17/150\t Time: 3.365\t Loss: 8.10658550\n","INFO:root:  Epoch 18/150\t Time: 3.304\t Loss: 7.79159077\n","INFO:root:  Epoch 19/150\t Time: 3.320\t Loss: 7.51498627\n","INFO:root:  Epoch 20/150\t Time: 3.340\t Loss: 7.27297161\n","INFO:root:  Epoch 21/150\t Time: 3.309\t Loss: 7.05682607\n","INFO:root:  Epoch 22/150\t Time: 3.355\t Loss: 6.86415768\n","INFO:root:  Epoch 23/150\t Time: 3.338\t Loss: 6.68891110\n","INFO:root:  Epoch 24/150\t Time: 3.334\t Loss: 6.52754084\n","INFO:root:  Epoch 25/150\t Time: 3.284\t Loss: 6.37832677\n","INFO:root:  Epoch 26/150\t Time: 3.332\t Loss: 6.24392086\n","INFO:root:  Epoch 27/150\t Time: 3.355\t Loss: 6.11712050\n","INFO:root:  Epoch 28/150\t Time: 3.330\t Loss: 6.00547371\n","INFO:root:  Epoch 29/150\t Time: 3.390\t Loss: 5.90013243\n","INFO:root:  Epoch 30/150\t Time: 3.350\t Loss: 5.79512584\n","INFO:root:  Epoch 31/150\t Time: 3.339\t Loss: 5.70159355\n","INFO:root:  Epoch 32/150\t Time: 3.333\t Loss: 5.60846298\n","INFO:root:  Epoch 33/150\t Time: 3.315\t Loss: 5.51957930\n","INFO:root:  Epoch 34/150\t Time: 3.296\t Loss: 5.43077326\n","INFO:root:  Epoch 35/150\t Time: 3.349\t Loss: 5.35634242\n","INFO:root:  Epoch 36/150\t Time: 3.335\t Loss: 5.27985927\n","INFO:root:  Epoch 37/150\t Time: 3.329\t Loss: 5.21015233\n","INFO:root:  Epoch 38/150\t Time: 3.328\t Loss: 5.14138305\n","INFO:root:  Epoch 39/150\t Time: 3.326\t Loss: 5.07345103\n","INFO:root:  Epoch 40/150\t Time: 3.360\t Loss: 5.00584690\n","INFO:root:  Epoch 41/150\t Time: 3.302\t Loss: 4.94095835\n","INFO:root:  Epoch 42/150\t Time: 3.338\t Loss: 4.87500857\n","INFO:root:  Epoch 43/150\t Time: 3.356\t Loss: 4.80224339\n","INFO:root:  Epoch 44/150\t Time: 3.377\t Loss: 4.73289814\n","INFO:root:  Epoch 45/150\t Time: 3.347\t Loss: 4.66570191\n","INFO:root:  Epoch 46/150\t Time: 3.374\t Loss: 4.60456664\n","INFO:root:  Epoch 47/150\t Time: 3.345\t Loss: 4.54360828\n","INFO:root:  Epoch 48/150\t Time: 3.326\t Loss: 4.48446429\n","INFO:root:  Epoch 49/150\t Time: 3.325\t Loss: 4.43150288\n","INFO:root:  Epoch 50/150\t Time: 3.317\t Loss: 4.39916975\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:417: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  \"please use `get_last_lr()`.\", UserWarning)\n","INFO:root:  LR scheduler: new learning rate is 1e-05\n","INFO:root:  Epoch 51/150\t Time: 3.307\t Loss: 4.39626895\n","INFO:root:  Epoch 52/150\t Time: 3.273\t Loss: 4.38837537\n","INFO:root:  Epoch 53/150\t Time: 3.335\t Loss: 4.38242194\n","INFO:root:  Epoch 54/150\t Time: 3.335\t Loss: 4.37770268\n","INFO:root:  Epoch 55/150\t Time: 3.385\t Loss: 4.37346775\n","INFO:root:  Epoch 56/150\t Time: 3.271\t Loss: 4.36389311\n","INFO:root:  Epoch 57/150\t Time: 3.310\t Loss: 4.36134695\n","INFO:root:  Epoch 58/150\t Time: 3.299\t Loss: 4.35418677\n","INFO:root:  Epoch 59/150\t Time: 3.348\t Loss: 4.34944053\n","INFO:root:  Epoch 60/150\t Time: 3.300\t Loss: 4.34190547\n","INFO:root:  Epoch 61/150\t Time: 3.294\t Loss: 4.33747822\n","INFO:root:  Epoch 62/150\t Time: 3.316\t Loss: 4.32976663\n","INFO:root:  Epoch 63/150\t Time: 3.364\t Loss: 4.32294455\n","INFO:root:  Epoch 64/150\t Time: 3.286\t Loss: 4.31770006\n","INFO:root:  Epoch 65/150\t Time: 3.292\t Loss: 4.31226996\n","INFO:root:  Epoch 66/150\t Time: 3.363\t Loss: 4.30609526\n","INFO:root:  Epoch 67/150\t Time: 3.288\t Loss: 4.29995924\n","INFO:root:  Epoch 68/150\t Time: 3.271\t Loss: 4.29175818\n","INFO:root:  Epoch 69/150\t Time: 3.286\t Loss: 4.28643328\n","INFO:root:  Epoch 70/150\t Time: 3.284\t Loss: 4.28103567\n","INFO:root:  Epoch 71/150\t Time: 3.265\t Loss: 4.27500079\n","INFO:root:  Epoch 72/150\t Time: 3.326\t Loss: 4.27060559\n","INFO:root:  Epoch 73/150\t Time: 3.296\t Loss: 4.25992164\n","INFO:root:  Epoch 74/150\t Time: 3.279\t Loss: 4.25554880\n","INFO:root:  Epoch 75/150\t Time: 3.291\t Loss: 4.24660555\n","INFO:root:  Epoch 76/150\t Time: 3.310\t Loss: 4.24136155\n","INFO:root:  Epoch 77/150\t Time: 3.298\t Loss: 4.23377897\n","INFO:root:  Epoch 78/150\t Time: 3.356\t Loss: 4.23177521\n","INFO:root:  Epoch 79/150\t Time: 3.324\t Loss: 4.21993971\n","INFO:root:  Epoch 80/150\t Time: 3.307\t Loss: 4.21516197\n","INFO:root:  Epoch 81/150\t Time: 3.333\t Loss: 4.20671109\n","INFO:root:  Epoch 82/150\t Time: 3.348\t Loss: 4.20312003\n","INFO:root:  Epoch 83/150\t Time: 3.340\t Loss: 4.19194123\n","INFO:root:  Epoch 84/150\t Time: 3.320\t Loss: 4.18593559\n","INFO:root:  Epoch 85/150\t Time: 3.345\t Loss: 4.17743158\n","INFO:root:  Epoch 86/150\t Time: 3.274\t Loss: 4.17368573\n","INFO:root:  Epoch 87/150\t Time: 3.325\t Loss: 4.16496515\n","INFO:root:  Epoch 88/150\t Time: 3.259\t Loss: 4.15972963\n","INFO:root:  Epoch 89/150\t Time: 3.358\t Loss: 4.14852568\n","INFO:root:  Epoch 90/150\t Time: 3.282\t Loss: 4.14525803\n","INFO:root:  Epoch 91/150\t Time: 3.350\t Loss: 4.13538936\n","INFO:root:  Epoch 92/150\t Time: 3.311\t Loss: 4.12662042\n","INFO:root:  Epoch 93/150\t Time: 3.306\t Loss: 4.11754742\n","INFO:root:  Epoch 94/150\t Time: 3.348\t Loss: 4.11053175\n","INFO:root:  Epoch 95/150\t Time: 3.272\t Loss: 4.10591432\n","INFO:root:  Epoch 96/150\t Time: 3.301\t Loss: 4.09658684\n","INFO:root:  Epoch 97/150\t Time: 3.328\t Loss: 4.09043242\n","INFO:root:  Epoch 98/150\t Time: 3.258\t Loss: 4.08466527\n","INFO:root:  Epoch 99/150\t Time: 3.273\t Loss: 4.07353377\n","INFO:root:  Epoch 100/150\t Time: 3.302\t Loss: 4.06583097\n","INFO:root:  Epoch 101/150\t Time: 3.350\t Loss: 4.05866639\n","INFO:root:  Epoch 102/150\t Time: 3.314\t Loss: 4.05465619\n","INFO:root:  Epoch 103/150\t Time: 3.310\t Loss: 4.04503057\n","INFO:root:  Epoch 104/150\t Time: 3.301\t Loss: 4.03740569\n","INFO:root:  Epoch 105/150\t Time: 3.288\t Loss: 4.02850442\n","INFO:root:  Epoch 106/150\t Time: 3.302\t Loss: 4.02037451\n","INFO:root:  Epoch 107/150\t Time: 3.307\t Loss: 4.01268595\n","INFO:root:  Epoch 108/150\t Time: 3.326\t Loss: 4.00452513\n","INFO:root:  Epoch 109/150\t Time: 3.327\t Loss: 3.99495429\n","INFO:root:  Epoch 110/150\t Time: 3.353\t Loss: 3.98887556\n","INFO:root:  Epoch 111/150\t Time: 3.299\t Loss: 3.97905973\n","INFO:root:  Epoch 112/150\t Time: 3.341\t Loss: 3.97452814\n","INFO:root:  Epoch 113/150\t Time: 3.319\t Loss: 3.96360207\n","INFO:root:  Epoch 114/150\t Time: 3.309\t Loss: 3.95748096\n","INFO:root:  Epoch 115/150\t Time: 3.279\t Loss: 3.95242037\n","INFO:root:  Epoch 116/150\t Time: 3.374\t Loss: 3.94335438\n","INFO:root:  Epoch 117/150\t Time: 3.323\t Loss: 3.93445186\n","INFO:root:  Epoch 118/150\t Time: 3.385\t Loss: 3.92705610\n","INFO:root:  Epoch 119/150\t Time: 3.357\t Loss: 3.92044705\n","INFO:root:  Epoch 120/150\t Time: 3.288\t Loss: 3.90983455\n","INFO:root:  Epoch 121/150\t Time: 3.304\t Loss: 3.89934777\n","INFO:root:  Epoch 122/150\t Time: 3.339\t Loss: 3.89561499\n","INFO:root:  Epoch 123/150\t Time: 3.304\t Loss: 3.88659109\n","INFO:root:  Epoch 124/150\t Time: 3.294\t Loss: 3.87810648\n","INFO:root:  Epoch 125/150\t Time: 3.290\t Loss: 3.87310068\n","INFO:root:  Epoch 126/150\t Time: 3.304\t Loss: 3.86131563\n","INFO:root:  Epoch 127/150\t Time: 3.322\t Loss: 3.85495244\n","INFO:root:  Epoch 128/150\t Time: 3.331\t Loss: 3.84468700\n","INFO:root:  Epoch 129/150\t Time: 3.309\t Loss: 3.83768283\n","INFO:root:  Epoch 130/150\t Time: 3.325\t Loss: 3.82940111\n","INFO:root:  Epoch 131/150\t Time: 3.346\t Loss: 3.82267683\n","INFO:root:  Epoch 132/150\t Time: 3.310\t Loss: 3.81258780\n","INFO:root:  Epoch 133/150\t Time: 3.323\t Loss: 3.80480729\n","INFO:root:  Epoch 134/150\t Time: 3.327\t Loss: 3.79729005\n","INFO:root:  Epoch 135/150\t Time: 3.366\t Loss: 3.78988805\n","INFO:root:  Epoch 136/150\t Time: 3.351\t Loss: 3.77942732\n","INFO:root:  Epoch 137/150\t Time: 3.355\t Loss: 3.77402202\n","INFO:root:  Epoch 138/150\t Time: 3.317\t Loss: 3.76603651\n","INFO:root:  Epoch 139/150\t Time: 3.301\t Loss: 3.75507111\n","INFO:root:  Epoch 140/150\t Time: 3.348\t Loss: 3.74871437\n","INFO:root:  Epoch 141/150\t Time: 3.290\t Loss: 3.74054872\n","INFO:root:  Epoch 142/150\t Time: 3.292\t Loss: 3.73624949\n","INFO:root:  Epoch 143/150\t Time: 3.323\t Loss: 3.72453463\n","INFO:root:  Epoch 144/150\t Time: 3.328\t Loss: 3.71737894\n","INFO:root:  Epoch 145/150\t Time: 3.316\t Loss: 3.70681710\n","INFO:root:  Epoch 146/150\t Time: 3.322\t Loss: 3.70095345\n","INFO:root:  Epoch 147/150\t Time: 3.311\t Loss: 3.69200882\n","INFO:root:  Epoch 148/150\t Time: 3.348\t Loss: 3.68645210\n","INFO:root:  Epoch 149/150\t Time: 3.328\t Loss: 3.67525961\n","INFO:root:  Epoch 150/150\t Time: 3.331\t Loss: 3.66950376\n","INFO:root:Pretraining time: 498.231\n","INFO:root:Finished pretraining.\n","INFO:root:Testing autoencoder...\n","/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:67: UserWarning: test_data has been renamed data\n","  warnings.warn(\"test_data has been renamed data\")\n","/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:57: UserWarning: test_labels has been renamed targets\n","  warnings.warn(\"test_labels has been renamed targets\")\n","INFO:root:Test set Loss: 5.62590389\n","INFO:root:Test set AUC: 84.04%\n","INFO:root:Autoencoder testing time: 4.194\n","INFO:root:Finished testing autoencoder.\n","INFO:root:Training optimizer: adam\n","INFO:root:Training learning rate: 0.0001\n","INFO:root:Training epochs: 150\n","INFO:root:Training learning rate scheduler milestones: (50,)\n","INFO:root:Training batch size: 200\n","INFO:root:Training weight decay: 5e-07\n","INFO:root:Initializing center c...\n","/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:62: UserWarning: train_data has been renamed data\n","  warnings.warn(\"train_data has been renamed data\")\n","/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:52: UserWarning: train_labels has been renamed targets\n","  warnings.warn(\"train_labels has been renamed targets\")\n","INFO:root:Center c initialized.\n","INFO:root:Starting training...\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n","INFO:root:  Epoch 1/150\t Time: 2.805\t Loss: 0.80015568\n","INFO:root:  Epoch 2/150\t Time: 2.847\t Loss: 0.49568549\n","INFO:root:  Epoch 3/150\t Time: 2.810\t Loss: 0.33583526\n","INFO:root:  Epoch 4/150\t Time: 2.840\t Loss: 0.24506985\n","INFO:root:  Epoch 5/150\t Time: 2.877\t Loss: 0.19178348\n","INFO:root:  Epoch 6/150\t Time: 2.825\t Loss: 0.15661201\n","INFO:root:  Epoch 7/150\t Time: 2.829\t Loss: 0.13166771\n","INFO:root:  Epoch 8/150\t Time: 2.876\t Loss: 0.11345168\n","INFO:root:  Epoch 9/150\t Time: 2.837\t Loss: 0.09911128\n","INFO:root:  Epoch 10/150\t Time: 2.833\t Loss: 0.08796845\n","INFO:root:  Epoch 11/150\t Time: 2.803\t Loss: 0.07785438\n","INFO:root:  Epoch 12/150\t Time: 2.794\t Loss: 0.07098582\n","INFO:root:  Epoch 13/150\t Time: 2.806\t Loss: 0.06433736\n","INFO:root:  Epoch 14/150\t Time: 2.830\t Loss: 0.05917841\n","INFO:root:  Epoch 15/150\t Time: 2.826\t Loss: 0.05501957\n","INFO:root:  Epoch 16/150\t Time: 2.824\t Loss: 0.05062790\n","INFO:root:  Epoch 17/150\t Time: 2.829\t Loss: 0.04680525\n","INFO:root:  Epoch 18/150\t Time: 2.833\t Loss: 0.04500361\n","INFO:root:  Epoch 19/150\t Time: 2.823\t Loss: 0.04240115\n","INFO:root:  Epoch 20/150\t Time: 2.785\t Loss: 0.03973367\n","INFO:root:  Epoch 21/150\t Time: 2.761\t Loss: 0.03716048\n","INFO:root:  Epoch 22/150\t Time: 2.824\t Loss: 0.03652995\n","INFO:root:  Epoch 23/150\t Time: 2.855\t Loss: 0.03429714\n","INFO:root:  Epoch 24/150\t Time: 2.814\t Loss: 0.03207147\n","INFO:root:  Epoch 25/150\t Time: 2.789\t Loss: 0.03083160\n","INFO:root:  Epoch 26/150\t Time: 2.873\t Loss: 0.03044757\n","INFO:root:  Epoch 27/150\t Time: 2.805\t Loss: 0.02892167\n","INFO:root:  Epoch 28/150\t Time: 2.780\t Loss: 0.02914415\n","INFO:root:  Epoch 29/150\t Time: 2.810\t Loss: 0.02706873\n","INFO:root:  Epoch 30/150\t Time: 2.826\t Loss: 0.02546907\n","INFO:root:  Epoch 31/150\t Time: 2.776\t Loss: 0.02443077\n","INFO:root:  Epoch 32/150\t Time: 2.825\t Loss: 0.02373029\n","INFO:root:  Epoch 33/150\t Time: 2.859\t Loss: 0.02270752\n","INFO:root:  Epoch 34/150\t Time: 2.843\t Loss: 0.02181125\n","INFO:root:  Epoch 35/150\t Time: 2.809\t Loss: 0.02093375\n","INFO:root:  Epoch 36/150\t Time: 2.786\t Loss: 0.02078234\n","INFO:root:  Epoch 37/150\t Time: 2.839\t Loss: 0.02049117\n","INFO:root:  Epoch 38/150\t Time: 2.816\t Loss: 0.02094791\n","INFO:root:  Epoch 39/150\t Time: 2.801\t Loss: 0.01901534\n","INFO:root:  Epoch 40/150\t Time: 2.789\t Loss: 0.01830624\n","INFO:root:  Epoch 41/150\t Time: 2.824\t Loss: 0.01851503\n","INFO:root:  Epoch 42/150\t Time: 2.821\t Loss: 0.01800828\n","INFO:root:  Epoch 43/150\t Time: 2.776\t Loss: 0.01809132\n","INFO:root:  Epoch 44/150\t Time: 2.788\t Loss: 0.01766715\n","INFO:root:  Epoch 45/150\t Time: 2.814\t Loss: 0.01738234\n","INFO:root:  Epoch 46/150\t Time: 2.798\t Loss: 0.01610180\n","INFO:root:  Epoch 47/150\t Time: 2.820\t Loss: 0.01561880\n","INFO:root:  Epoch 48/150\t Time: 2.784\t Loss: 0.01572207\n","INFO:root:  Epoch 49/150\t Time: 2.825\t Loss: 0.01550928\n","INFO:root:  Epoch 50/150\t Time: 2.777\t Loss: 0.01491411\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:417: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  \"please use `get_last_lr()`.\", UserWarning)\n","INFO:root:  LR scheduler: new learning rate is 1e-05\n","INFO:root:  Epoch 51/150\t Time: 2.812\t Loss: 0.01429783\n","INFO:root:  Epoch 52/150\t Time: 2.782\t Loss: 0.01408270\n","INFO:root:  Epoch 53/150\t Time: 2.796\t Loss: 0.01407466\n","INFO:root:  Epoch 54/150\t Time: 2.832\t Loss: 0.01423620\n","INFO:root:  Epoch 55/150\t Time: 2.763\t Loss: 0.01385450\n","INFO:root:  Epoch 56/150\t Time: 2.825\t Loss: 0.01385104\n","INFO:root:  Epoch 57/150\t Time: 2.767\t Loss: 0.01383296\n","INFO:root:  Epoch 58/150\t Time: 2.800\t Loss: 0.01418095\n","INFO:root:  Epoch 59/150\t Time: 2.812\t Loss: 0.01413658\n","INFO:root:  Epoch 60/150\t Time: 2.784\t Loss: 0.01441808\n","INFO:root:  Epoch 61/150\t Time: 2.788\t Loss: 0.01383283\n","INFO:root:  Epoch 62/150\t Time: 2.809\t Loss: 0.01372651\n","INFO:root:  Epoch 63/150\t Time: 2.827\t Loss: 0.01396736\n","INFO:root:  Epoch 64/150\t Time: 2.820\t Loss: 0.01356699\n","INFO:root:  Epoch 65/150\t Time: 2.814\t Loss: 0.01398368\n","INFO:root:  Epoch 66/150\t Time: 2.827\t Loss: 0.01348201\n","INFO:root:  Epoch 67/150\t Time: 2.754\t Loss: 0.01350220\n","INFO:root:  Epoch 68/150\t Time: 2.802\t Loss: 0.01371941\n","INFO:root:  Epoch 69/150\t Time: 2.779\t Loss: 0.01318474\n","INFO:root:  Epoch 70/150\t Time: 2.787\t Loss: 0.01359321\n","INFO:root:  Epoch 71/150\t Time: 2.831\t Loss: 0.01376138\n","INFO:root:  Epoch 72/150\t Time: 2.808\t Loss: 0.01338968\n","INFO:root:  Epoch 73/150\t Time: 2.824\t Loss: 0.01386214\n","INFO:root:  Epoch 74/150\t Time: 2.849\t Loss: 0.01379532\n","INFO:root:  Epoch 75/150\t Time: 2.825\t Loss: 0.01332118\n","INFO:root:  Epoch 76/150\t Time: 2.852\t Loss: 0.01313419\n","INFO:root:  Epoch 77/150\t Time: 2.829\t Loss: 0.01440378\n","INFO:root:  Epoch 78/150\t Time: 2.803\t Loss: 0.01418555\n","INFO:root:  Epoch 79/150\t Time: 2.780\t Loss: 0.01373001\n","INFO:root:  Epoch 80/150\t Time: 2.779\t Loss: 0.01287528\n","INFO:root:  Epoch 81/150\t Time: 2.783\t Loss: 0.01350621\n","INFO:root:  Epoch 82/150\t Time: 2.793\t Loss: 0.01345639\n","INFO:root:  Epoch 83/150\t Time: 2.826\t Loss: 0.01291127\n","INFO:root:  Epoch 84/150\t Time: 2.853\t Loss: 0.01418529\n","INFO:root:  Epoch 85/150\t Time: 2.810\t Loss: 0.01337069\n","INFO:root:  Epoch 86/150\t Time: 2.783\t Loss: 0.01307942\n","INFO:root:  Epoch 87/150\t Time: 2.750\t Loss: 0.01323804\n","INFO:root:  Epoch 88/150\t Time: 2.886\t Loss: 0.01296365\n","INFO:root:  Epoch 89/150\t Time: 2.796\t Loss: 0.01314107\n","INFO:root:  Epoch 90/150\t Time: 2.828\t Loss: 0.01293361\n","INFO:root:  Epoch 91/150\t Time: 2.815\t Loss: 0.01324288\n","INFO:root:  Epoch 92/150\t Time: 2.801\t Loss: 0.01307103\n","INFO:root:  Epoch 93/150\t Time: 2.774\t Loss: 0.01284652\n","INFO:root:  Epoch 94/150\t Time: 2.789\t Loss: 0.01286675\n","INFO:root:  Epoch 95/150\t Time: 2.835\t Loss: 0.01312252\n","INFO:root:  Epoch 96/150\t Time: 2.807\t Loss: 0.01251647\n","INFO:root:  Epoch 97/150\t Time: 2.822\t Loss: 0.01299421\n","INFO:root:  Epoch 98/150\t Time: 2.817\t Loss: 0.01220440\n","INFO:root:  Epoch 99/150\t Time: 2.841\t Loss: 0.01260701\n","INFO:root:  Epoch 100/150\t Time: 2.820\t Loss: 0.01252198\n","INFO:root:  Epoch 101/150\t Time: 2.810\t Loss: 0.01310119\n","INFO:root:  Epoch 102/150\t Time: 2.823\t Loss: 0.01271148\n","INFO:root:  Epoch 103/150\t Time: 2.840\t Loss: 0.01222791\n","INFO:root:  Epoch 104/150\t Time: 2.809\t Loss: 0.01228748\n","INFO:root:  Epoch 105/150\t Time: 2.785\t Loss: 0.01254726\n","INFO:root:  Epoch 106/150\t Time: 2.820\t Loss: 0.01257280\n","INFO:root:  Epoch 107/150\t Time: 2.811\t Loss: 0.01195779\n","INFO:root:  Epoch 108/150\t Time: 2.895\t Loss: 0.01204971\n","INFO:root:  Epoch 109/150\t Time: 2.836\t Loss: 0.01242108\n","INFO:root:  Epoch 110/150\t Time: 2.809\t Loss: 0.01190204\n","INFO:root:  Epoch 111/150\t Time: 2.774\t Loss: 0.01231337\n","INFO:root:  Epoch 112/150\t Time: 2.821\t Loss: 0.01223787\n","INFO:root:  Epoch 113/150\t Time: 2.820\t Loss: 0.01227363\n","INFO:root:  Epoch 114/150\t Time: 2.813\t Loss: 0.01215465\n","INFO:root:  Epoch 115/150\t Time: 2.764\t Loss: 0.01237438\n","INFO:root:  Epoch 116/150\t Time: 2.764\t Loss: 0.01243641\n","INFO:root:  Epoch 117/150\t Time: 2.814\t Loss: 0.01165463\n","INFO:root:  Epoch 118/150\t Time: 2.836\t Loss: 0.01156778\n","INFO:root:  Epoch 119/150\t Time: 2.787\t Loss: 0.01176892\n","INFO:root:  Epoch 120/150\t Time: 2.835\t Loss: 0.01196163\n","INFO:root:  Epoch 121/150\t Time: 2.810\t Loss: 0.01198850\n","INFO:root:  Epoch 122/150\t Time: 2.772\t Loss: 0.01176023\n","INFO:root:  Epoch 123/150\t Time: 2.787\t Loss: 0.01141646\n","INFO:root:  Epoch 124/150\t Time: 2.863\t Loss: 0.01180179\n","INFO:root:  Epoch 125/150\t Time: 2.791\t Loss: 0.01153665\n","INFO:root:  Epoch 126/150\t Time: 2.807\t Loss: 0.01169809\n","INFO:root:  Epoch 127/150\t Time: 2.794\t Loss: 0.01136452\n","INFO:root:  Epoch 128/150\t Time: 2.826\t Loss: 0.01177393\n","INFO:root:  Epoch 129/150\t Time: 2.818\t Loss: 0.01192233\n","INFO:root:  Epoch 130/150\t Time: 2.807\t Loss: 0.01181094\n","INFO:root:  Epoch 131/150\t Time: 2.839\t Loss: 0.01122283\n","INFO:root:  Epoch 132/150\t Time: 2.838\t Loss: 0.01098196\n","INFO:root:  Epoch 133/150\t Time: 2.787\t Loss: 0.01144911\n","INFO:root:  Epoch 134/150\t Time: 2.762\t Loss: 0.01146981\n","INFO:root:  Epoch 135/150\t Time: 2.923\t Loss: 0.01164240\n","INFO:root:  Epoch 136/150\t Time: 2.870\t Loss: 0.01136847\n","INFO:root:  Epoch 137/150\t Time: 2.814\t Loss: 0.01111997\n","INFO:root:  Epoch 138/150\t Time: 2.792\t Loss: 0.01124464\n","INFO:root:  Epoch 139/150\t Time: 2.815\t Loss: 0.01118326\n","INFO:root:  Epoch 140/150\t Time: 2.785\t Loss: 0.01146315\n","INFO:root:  Epoch 141/150\t Time: 2.835\t Loss: 0.01134584\n","INFO:root:  Epoch 142/150\t Time: 2.755\t Loss: 0.01094313\n","INFO:root:  Epoch 143/150\t Time: 2.780\t Loss: 0.01115527\n","INFO:root:  Epoch 144/150\t Time: 2.796\t Loss: 0.01117752\n","INFO:root:  Epoch 145/150\t Time: 2.849\t Loss: 0.01162049\n","INFO:root:  Epoch 146/150\t Time: 2.809\t Loss: 0.01075928\n","INFO:root:  Epoch 147/150\t Time: 2.817\t Loss: 0.01135159\n","INFO:root:  Epoch 148/150\t Time: 2.769\t Loss: 0.01099288\n","INFO:root:  Epoch 149/150\t Time: 2.769\t Loss: 0.01089875\n","INFO:root:  Epoch 150/150\t Time: 2.812\t Loss: 0.01080139\n","INFO:root:Training time: 421.825\n","INFO:root:Finished training.\n","INFO:root:Starting testing...\n","/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:67: UserWarning: test_data has been renamed data\n","  warnings.warn(\"test_data has been renamed data\")\n","/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:57: UserWarning: test_labels has been renamed targets\n","  warnings.warn(\"test_labels has been renamed targets\")\n","INFO:root:Testing time: 3.894\n","INFO:root:Test set AUC: 90.35%\n","INFO:root:Finished testing.\n","/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:67: UserWarning: test_data has been renamed data\n","  warnings.warn(\"test_data has been renamed data\")\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"lH8_QpdbsjWC"},"execution_count":null,"outputs":[]}]}